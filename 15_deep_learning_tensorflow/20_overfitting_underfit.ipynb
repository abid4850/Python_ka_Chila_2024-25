{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting vs. Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 18ms/step - accuracy: 0.8123 - loss: 7.8248 - val_accuracy: 0.8663 - val_loss: 0.5784\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.8845 - loss: 0.4713 - val_accuracy: 0.9099 - val_loss: 0.4384\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 15ms/step - accuracy: 0.9207 - loss: 0.3147 - val_accuracy: 0.9220 - val_loss: 0.3307\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 15ms/step - accuracy: 0.9342 - loss: 0.2590 - val_accuracy: 0.9320 - val_loss: 0.2921\n",
      "Epoch 5/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 14ms/step - accuracy: 0.9401 - loss: 0.2390 - val_accuracy: 0.9394 - val_loss: 0.2690\n",
      "Epoch 6/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 15ms/step - accuracy: 0.9471 - loss: 0.1999 - val_accuracy: 0.9407 - val_loss: 0.2693\n",
      "Epoch 7/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.9493 - loss: 0.2071 - val_accuracy: 0.9433 - val_loss: 0.2633\n",
      "Epoch 8/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 13ms/step - accuracy: 0.9548 - loss: 0.1855 - val_accuracy: 0.9401 - val_loss: 0.2815\n",
      "Epoch 9/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 16ms/step - accuracy: 0.9577 - loss: 0.1631 - val_accuracy: 0.9480 - val_loss: 0.2412\n",
      "Epoch 10/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 15ms/step - accuracy: 0.9571 - loss: 0.1725 - val_accuracy: 0.9425 - val_loss: 0.3027\n",
      "CPU times: total: 3min 37s\n",
      "Wall time: 5min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ignore warnings in the output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Define a simple model\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "# Train the baseline model\n",
    "history_baseline = model.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run model on Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# laod and normalize the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Define a model for normalized data\n",
    "def create_model_with_norm_data():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_with_norm_data = create_model_with_norm_data()\n",
    "\n",
    "# Train the baseline model\n",
    "history_norm_data = model_with_norm_data.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "# plot the effect of normalization\n",
    "plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')\n",
    "plt.plot(history_norm_data.history['val_accuracy'], label = 'Normalized Validation Accuracy')\n",
    "plt.title('Effect of Normalization')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding Dropout\n",
    "Now, let's modify the model to include dropout and observe its effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model with dropout\n",
    "def create_model_with_dropout():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),  # Adding dropout\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model_with_dropout = create_model_with_dropout()\n",
    "\n",
    "# Train the model with dropout\n",
    "history_dropout = model_with_dropout.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Plotting the effect of dropout\n",
    "plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')\n",
    "# plot line for normalized data\n",
    "plt.plot(history_norm_data.history['val_accuracy'], label='Normalized Validation Accuracy')\n",
    "plt.plot(history_dropout.history['val_accuracy'], label='With Dropout Validation Accuracy')\n",
    "plt.title('Effect of Dropout on Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Early Stopping\n",
    "Finally, we will use early stopping to stop training when the validation loss starts to increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model_with_es = create_model_with_norm_data()\n",
    "\n",
    "# train with early stop\n",
    "history_es = model_with_es.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# plot the effect of early stopping\n",
    "plt.plot(history_baseline.history['val_accuracy'], label='Baseline Validation Accuracy')\n",
    "# plot line for normalized data\n",
    "plt.plot(history_norm_data.history['val_accuracy'], label='Normalized Validation Accuracy')\n",
    "plt.plot(history_dropout.history['val_accuracy'], label='With Dropout Validation Accuracy')\n",
    "plt.plot(history_es.history['val_accuracy'], label='With Early Stopping Validation Accuracy')\n",
    "plt.title('Effect of Early Stopping on Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. L1/L2 Regularization\n",
    "L1 and L2 regularization are two common regularization techniques that can be used to reduce overfitting.\n",
    "\n",
    "L1 in Keras: kernel_regularizer=regularizers.l1(0.001) L2 in Keras: kernel_regularizer=regularizers.l2(0.001)\n",
    "\n",
    "L1 changes the loss function by adding the sum of the absolute weights to it. L2 changes the loss function by adding the sum of the squared weights to it.\n",
    "\n",
    "L1 make zero some of the weights.\n",
    "L2 make the weights small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a model with L1 regularization\n",
    "def create_model_with_l1():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1(0.0001)),  # L1 regularization\n",
    "        Dropout(0.5), # Adding dropout which will also be regularized by L1\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define a model with L2 regularization\n",
    "def create_model_with_l2():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.0001)),  # L2 regularization\n",
    "        Dropout(0.5), # Adding dropout which will also be regularized by L2\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# should we combine them both?\n",
    "# Define a model with L1_L2 (combined) regularization\n",
    "def create_model_with_l1_l2():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.0001, l2=0.0001)),  # L1_L2 regularization\n",
    "        Dropout(0.5), # Adding dropout which will also be regularized by L1_L2\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare and train the models\n",
    "model_with_l1 = create_model_with_l1()\n",
    "history_l1 = model_with_l1.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "model_with_l2 = create_model_with_l2()\n",
    "history_l2 = model_with_l2.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "model_with_l1_l2 = create_model_with_l1_l2()\n",
    "history_l1_l2 = model_with_l1_l2.fit(x_train, y_train, epochs=10, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Plotting the effect of regularization\n",
    "# plot line for normalized data\n",
    "plt.plot(history_norm_data.history['val_accuracy'], label='Normalized Validation Accuracy')\n",
    "plt.plot(history_dropout.history['val_accuracy'], label='With Dropout Validation Accuracy')\n",
    "plt.plot(history_l1.history['val_accuracy'], label='With L1 Regularization Validation Accuracy')\n",
    "plt.plot(history_l2.history['val_accuracy'], label='With L2 Regularization Validation Accuracy')\n",
    "plt.plot(history_l1_l2.history['val_accuracy'], label='With L1_L2 Regularization Validation Accuracy')\n",
    "plt.title('Effect of Regularization on Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the effect of regularization\n",
    "# plot line for normalized data\n",
    "# plt.plot(history_norm_data.history['val_accuracy'], label='Normalized Validation Accuracy')\n",
    "# plt.plot(history_dropout.history['val_accuracy'], label='With Dropout Validation Accuracy')\n",
    "plt.plot(history_l1.history['val_accuracy'], label='With L1 Regularization Validation Accuracy')\n",
    "plt.plot(history_l2.history['val_accuracy'], label='With L2 Regularization Validation Accuracy')\n",
    "plt.plot(history_l1_l2.history['val_accuracy'], label='With L1_L2 Regularization Validation Accuracy')\n",
    "plt.title('Effect of Regularization on Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-fold cross validation\n",
    "k-fold cross-validation in TensorFlow requires a bit more manual setup compared to libraries like scikit-learn, which offer built-in k-fold cross-validation functions. TensorFlow does not have a direct function for k-fold cross-validation, but you can achieve this by manually splitting your dataset into k folds and then training and evaluating your model on each fold.\n",
    "\n",
    "Below is an example code that demonstrates how to perform k-fold cross-validation with the MNIST dataset in TensorFlow. This example uses a simple neural network similar to the ones discussed previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize data\n",
    "y_train = to_categorical(y_train, 10)  # One-hot encode labels\n",
    "\n",
    "# Define a simple model architecture function\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Flatten(input_shape=(28, 28)),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Prepare for k-fold cross-validation\n",
    "k = 5  # Number of folds\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Arrays to store scores\n",
    "fold_idx = 1\n",
    "scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(x_train):\n",
    "    # Split data\n",
    "    x_train_fold, x_val_fold = x_train[train_index], x_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    # create a model\n",
    "    model_kfold_cv = create_model()\n",
    "    \n",
    "    # Train model\n",
    "    history_kfold_cv = model_kfold_cv.fit(x_train_fold, y_train_fold, epochs=10, validation_data=(x_val_fold, y_val_fold), verbose=1)\n",
    "    \n",
    "    # Append the last score from the history\n",
    "    scores.append((history_kfold_cv.history['val_loss'][-1], history_kfold_cv.history['val_accuracy'][-1]))\n",
    "\n",
    "# Calculate and print average performance across all folds\n",
    "average_loss, average_accuracy = np.mean(scores, axis=0)\n",
    "print(f'\\nK-Fold Validation Results:\\nAverage Loss: {average_loss}, Average Accuracy: {average_accuracy}')\n",
    "\n",
    "# Plotting\n",
    "folds = range(1, k + 1)\n",
    "val_losses, val_accuracies = zip(*scores)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(folds, val_losses, 'o-', label='Validation Loss')\n",
    "plt.plot(folds, [average_loss] * k, 'r--', label='Average Loss')\n",
    "plt.title('Validation Loss per Fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(folds, val_accuracies, 'o-', label='Validation Accuracy')\n",
    "plt.plot(folds, [average_accuracy] * k, 'r--', label='Average Accuracy')\n",
    "plt.title('Validation Accuracy per Fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs the following steps:\n",
    "\n",
    "- 1. Normalizes the data and one-hot encodes the labels.\n",
    "- 2. Defines a simple neural network model inside a function so that a fresh model can be created for each fold.\n",
    "- 3. Uses KFold from sklearn.model_selection to generate train/validation splits.\n",
    "- 4. Trains a new model instance on each train fold and evaluates it on the corresponding validation fold.\n",
    "- 5. Calculates and prints the average loss and accuracy across all folds at the end.\n",
    "  \n",
    "Remember, k-fold cross-validation can be computationally expensive since it involves training and evaluating k separate models. Adjust the number of epochs, model complexity, or the dataset size as necessary to manage the computational load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tnf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
